# -*- coding: utf-8 -*-
"""html2json_batch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dxcBGgSeSRcnBg2YW7Dc1VdCqwmRKRNL
"""

# @title Импорт необходимых модулей
import os
import zipfile
import unicodedata
import glob

import json
from bs4 import BeautifulSoup

#@title Распаковка архива

zip_path = '/content/dictionary_output2.zip'

#новая папка создается на основе названия архива
archive_name = os.path.basename(zip_path).replace('.zip', '')
extract_folder = f'/content/{archive_name}_extracted'

os.makedirs(extract_folder, exist_ok=True)

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    for file_info in zip_ref.infolist():
        original_name = file_info.filename
        #нормализуем NFC при извлечении
        normalized_name = unicodedata.normalize('NFC', original_name)
        file_info.filename = normalized_name

        zip_ref.extract(file_info, extract_folder)
        print(f"Извлечен: {normalized_name}")

print(f"Архив извлечен в папку: {extract_folder}")

#@title Функция конвертации HTML -> JSON

def html_to_json(file_path, output_folder):
    """
    Конвертирует html словарную статью в json.

    Args:
        file_path: путь к html файлу
        output_folder: папка для сохранения файла json
    """
    headline_word = ""
    rest_of_the_article = ""

    #читаем HTML
    with open(file_path, 'r', encoding='utf-8') as f:
        html_content = f.read()

    soup = BeautifulSoup(html_content, 'html.parser')

    #ищем body
    for tag in soup.find_all():
        if tag.name == 'body':
            s = str(tag)
            break

    #ищем div внутри body
    soup2 = BeautifulSoup(s, 'html.parser')
    for tag2 in soup2.find_all():
        if tag2.name == 'div':
            s2 = str(tag2)
            break

    #собираем span из параграфов
    spans = []
    soup3 = BeautifulSoup(s2, 'html.parser')
    for tag3 in soup3.find_all():
        if tag3.name == 'p':
            s3 = str(tag3)
            soup4 = BeautifulSoup(s3, 'html.parser')
            for tag4 in soup4.find_all():
                if tag4.name == 'span':
                    spans.append(tag4)

    #разделяем заголовок и остальной текст
    check = 0
    for span in spans:
        style_attr = span['style']
        if check == 0:
            headline_word = headline_word + str(span)
        if check == 1:
            rest_of_the_article = rest_of_the_article + str(span)
     #первый жирный span - это заголовок
        if 'font-family:\'Times New Roman\'; font-weight:bold' in style_attr:
            check = 1

    #создание json структуры
    dic = {
        "headline": headline_word,
        "rest": rest_of_the_article
    }

    #еще раз нормализуем имя файла
    base_name = os.path.basename(file_path)  # получаем имя файла
    base_name = os.path.splitext(base_name)[0]  # убираем .html
    base_name = unicodedata.normalize('NFC', base_name)  # ФИКС для ударений!

    output_filename = f"{base_name}.json"
    output_path = os.path.join(output_folder, output_filename)

    #записываем json
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(dic, f, indent=4, ensure_ascii=False)

    return

# @title Пакетная обработка

input_folder = extract_folder
output_folder = '/content/json_output_results'

os.makedirs(output_folder, exist_ok=True)

for filename in os.listdir(input_folder):
    #пропуск служебных папок
    if filename.startswith('.'):
        continue

    file_path = os.path.join(input_folder, filename)

    #обработка только html файлов
    if os.path.isfile(file_path) and filename.endswith('.html'):
        #снова нормализуем имена для вывода
        normalized_filename = unicodedata.normalize('NFC', filename)
        print(f"Обрабатываем: {normalized_filename}")

        html_to_json(file_path, output_folder)

print(f"\nГотово! JSON файлы сохранены в: {output_folder}")